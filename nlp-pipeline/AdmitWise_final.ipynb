{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr-2AEV_IxfV",
        "outputId": "1a215e4c-4fb9-4575-f7e9-bc45797dc2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/232.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q5434jq-Eln",
        "outputId": "17a46697-8b51-4085-f944-a60d6bda5cc3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.23.5-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.5 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.23.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.5 PyMuPDFb-1.23.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yprhFqhl-m8p",
        "outputId": "5a1aba46-2d87-44ea-ed6c-70187188f114"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.0.6-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (9.4.0)\n",
            "Installing collected packages: reportlab\n",
            "Successfully installed reportlab-4.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rMY4i---qDj",
        "outputId": "25d92936-3a3c-4cec-8f7e-9af7675f01ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHbNNK4nJ55k",
        "outputId": "7b8676c5-47c7-42ca-fede-be6c8b507887"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import pandas as pd\n",
        "\n",
        "# Define a function to extract text from a PDF file in Google Drive\n",
        "def extract_text_from_drive_pdf(drive_path):\n",
        "    text = \"\"\n",
        "    with open('/content/gdrive/My Drive/' + drive_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Process your Google Drive PDF files and extract text\n",
        "drive_pdf_paths = [\"Hacking/Data/akoehler-sop.pdf\"]\n",
        "text_data = []\n",
        "for drive_path in drive_pdf_paths:\n",
        "    text_data.append(extract_text_from_drive_pdf(drive_path))\n",
        "\n",
        "# Create a dataset\n",
        "data = {'Text': text_data}\n",
        "df = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "PTsmwh-vKIlX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code is just finding 20 important words per document**\n",
        "\n"
      ],
      "metadata": {
        "id": "U7S4bWUU95GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "import re\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Function to extract text from a PDF file in Google Drive\n",
        "def extract_text_from_drive_pdf(drive_path):\n",
        "    text = \"\"\n",
        "    with open('/content/gdrive/My Drive/' + drive_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to process all PDF files in a folder\n",
        "def process_pdfs_in_folder(folder_path):\n",
        "    sop_texts = []\n",
        "    for file in os.listdir('/content/gdrive/My Drive/' + folder_path):\n",
        "        if file.endswith('.pdf'):\n",
        "            sop_texts.append(extract_text_from_drive_pdf(os.path.join(folder_path, file)))\n",
        "    return sop_texts\n",
        "\n",
        "# Specify the folder path where your PDFs are located in Google Drive\n",
        "pdf_folder_path = \"Hacking/sop-data\"\n",
        "\n",
        "# Process SOP PDFs in the specified folder\n",
        "sop_texts = process_pdfs_in_folder(pdf_folder_path)\n",
        "\n",
        "# Use TF-IDF to find important words in each SOP\n",
        "# Use TF-IDF to find important words in each SOP\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, smooth_idf=True)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sop_texts)\n",
        "\n",
        "\n",
        "# Get the feature names (words) corresponding to the TF-IDF matrix\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Find the most important words in each SOP\n",
        "important_words_per_sop = []\n",
        "for i, sop_text in enumerate(sop_texts):\n",
        "    tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
        "    important_word_indices = tfidf_scores.argsort()[-20:][::-1]  # Top 20 important words\n",
        "    important_words = [feature_names[idx] for idx in important_word_indices]\n",
        "    important_words_per_sop.append(important_words)\n",
        "\n",
        "# Print important words for each SOP\n",
        "for i, words in enumerate(important_words_per_sop):\n",
        "    print(f\"Important words in SOP {i + 1}:\", words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ1YfNFuXO0n",
        "outputId": "e9a85c77-db36-442f-eaac-cf3c7d45f712"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Important words in SOP 1: ['computer', 'artificial', 'science', 'intelligence', 'field', 'projects', 'graphics', 'depth', 'research', 'areas', 'knowledge', 'users', 'undergraduate', 'xinu', 'development', 'project', 'hope', 'software', 'feel', 'degree']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHcWSwbjhoak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is highlighting the general text based on Text importance**"
      ],
      "metadata": {
        "id": "H12Hxj1SlQe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.platypus import Table, TableStyle\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access your files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define a function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        for page in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "# Define a function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "# Define a function to calculate TF-IDF scores\n",
        "def calculate_tfidf_scores(sentences):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "    return tfidf_matrix\n",
        "\n",
        "# Define a function to calculate TextRank scores\n",
        "def calculate_textrank_scores(sentences, threshold=0.2, max_iterations=100):\n",
        "    def sentence_similarity(sent1, sent2):\n",
        "        sent1 = nltk.word_tokenize(sent1)\n",
        "        sent2 = nltk.word_tokenize(sent2)\n",
        "        all_words = list(set(sent1 + sent2))\n",
        "        vector1 = [0] * len(all_words)\n",
        "        vector2 = [0] * len(all_words)\n",
        "\n",
        "        for w in sent1:\n",
        "            vector1[all_words.index(w)] += 1\n",
        "\n",
        "        for w in sent2:\n",
        "            vector2[all_words.index(w)] += 1\n",
        "\n",
        "        return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "    sentence_similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        for j in range(len(sentences)):\n",
        "            if i != j:\n",
        "                sentence_similarity_matrix[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
        "\n",
        "    scores = np.zeros(len(sentences))\n",
        "    for _ in range(max_iterations):\n",
        "        for i in range(len(sentences)):\n",
        "            score = 0\n",
        "            for j in range(len(sentences)):\n",
        "                if i != j:\n",
        "                    score += sentence_similarity_matrix[i][j] / sum(sentence_similarity_matrix[j])\n",
        "            scores[i] = (1 - threshold) + threshold * score\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Define a function to highlight important sentences in the PDF\n",
        "def highlight_important_sentences(pdf_path, important_sentences, output_path):\n",
        "    doc = SimpleDocTemplate(output_path, pagesize=letter)\n",
        "    story = []\n",
        "\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    sentences = preprocess_text(text)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence in important_sentences:\n",
        "            p = Paragraph(\"<u>{}</u>\".format(sentence), getSampleStyleSheet()['Normal'])\n",
        "        else:\n",
        "            p = Paragraph(sentence, getSampleStyleSheet()['Normal'])\n",
        "        story.append(p)\n",
        "        story.append(Spacer(1, 12))\n",
        "    doc.build(story)\n",
        "\n",
        "# Process all PDF files in the \"Hacking/Data\" directory in your Google Drive\n",
        "data_directory = '/content/drive/My Drive/Hacking/sop-data'\n",
        "output_directory = '/content/drive/My Drive/Hacking/General'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "# Iterate through all PDF files in the data directory\n",
        "for pdf_filename in os.listdir(data_directory):\n",
        "    if pdf_filename.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(data_directory, pdf_filename)\n",
        "\n",
        "        # Extract important sentences using TextRank\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        sentences = preprocess_text(text)\n",
        "        textrank_scores = calculate_textrank_scores(sentences)\n",
        "        important_sentences_textrank = [sentences[i] for i in np.where(textrank_scores > np.mean(textrank_scores))[0]]\n",
        "\n",
        "        print(important_sentences_textrank)\n",
        "\n",
        "        # Generate the highlighted PDF\n",
        "        highlighted_pdf_path = os.path.join(output_directory, \"highlighted_\" + pdf_filename)\n",
        "        highlight_important_sentences(pdf_path, important_sentences_textrank, highlighted_pdf_path)\n",
        "\n",
        "        print(\"Highlighted PDF saved as:\", highlighted_pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5fZx9uGd4Jf",
        "outputId": "7b2da2f7-aae0-4b36-80c2-88541fb2f6f2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['Adam Koehler \\nStatement of Purpose \\n \\n After taking a multitude of computer science courses over my academic career I \\nhave perceived that most of the courses tend to build a student’s knowledge of comput er \\nscience outward, broadening familiarity over several areas of computer s cience rather \\nthan creating a concentrated depth of knowledge in any one topic.', 'By \\nstudying these areas in depth, I hope to resolve the disparity between ha ving a wide scope \\nof computer science knowledge and having a more concentrated understanding of a \\nsingle computer science field.', 'The attraction of artificial intelligence for me lies in its breadt h of applicability, both as a \\nmethod of problem solving in itself and in a symbiotic integration with other ar eas of \\ncomputer science.', 'A broad spectrum of applications exist within the  artificial \\nintelligence field, ranging from intelligent non-player controlled chara cters in computer \\ngame software to a ubiquitous computing solution that intelligently reacts  to a variety of \\nusers.', \"While I have striven to develop my understanding of art ificial \\nintelligence during my undergraduate education, the choreographed requireme nts of a \\nbachelor's degree have restricted my research to only a minute sample o f artificial \\nintelligence’s applications.\", 'During my exposure to the field, I have often b een \\nunsatisfied with the level of interaction artificial intelligence displa ys in response to \\nprompts of varying complexity.', 'I do not believe the field has been develop ed to its \\npotential in any regard, and feel that considerable progress can be made to i mprove the \\ninteractive experience that users have with an artificial intellig ence application.', 'This \\ngenuine intrigue combined with my curiosity for the subject matter and the l imitless \\npotential of the field are the reason why I wish to pursue a greater depth of  knowledge in \\nartificial intelligence.', 'Through the education gained in pursuit of a master ’s degree, I \\nhope to be able to enrich the authenticity of many artificial intell igence experiences, from \\ncomputer games, to interactive toys and beyond.', 'While artificial intelligence holds the most intrigue for me, a second ary area of \\ninterest is computer graphics.', 'Akin to artificial intelligence in that I have only touched \\non the subject during my undergraduate career, I hope to explore the areas of two and \\nthree dimensional rendering more acutely.', 'While I have always enjoyed the  freedom of \\ncreative expression, and embrace its value in many aspects of probl em solving in \\ncomputer science, the rigidity of programming has precluded my pursuit  of many artistic \\ninterests.', 'I feel that a more in-depth review of the field of computer gra phics would be \\nvery fulfilling for me, both for its creative liberties and for its appli cation to other areas of \\ncomputer science.', 'I believe that a deeper appreciation for the s tate-of-the-art in computer graphics will only help me in all future computer science purs uits, and I look \\nforward to the new challenges while pursuing my master’s degree in comp uter science.', 'My undergraduate education has prepared me for the depth and commitment \\nrequired of graduate research in the field of computer science.', 'Two projects  I have \\nparticipated in would be research with the Embedded XINU team, and a senior  design \\nproject also involving the XINU operating system.', 'While participati ng on the Embedded \\nXINU team I have chosen to aide in the research and development of an external a nalysis \\ntool for embedded systems development.', 'Once complete, it will allow users to a ccess many \\ndebugging features, such as single stepping through code, breaking at a cer tain point in \\nthe code, and continuing the execution of the code from the breakpoint.', 'This and several other projects have helped me deve lop a \\nwealth of knowledge in the computer science field, and have certainly broadened m y \\ninterests.', \"Through graduate studies I hope to explore the fields of artificial intell igence and \\ncomputer graphics more fully, and negotiate the requirements of intertwini ng the two \\nmore seamlessly than today's efforts produce.\", 'My ultimate goal is to produce meaningful \\nwork that combines the acquired in-depth knowledge of both the artificial intell igence \\nfield and the computer graphics field, while building on the solid foundation I have \\ncreated in my undergraduate pursuits.']\n",
            "None\n",
            "Highlighted PDF saved as: /content/drive/My Drive/Hacking/General/highlighted_akoehler-sop.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8JJB2dtplNFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taking UB's values into account**"
      ],
      "metadata": {
        "id": "w4HJUKUClaOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ... your previous imports ...\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from reportlab.lib.styles import ParagraphStyle  # Import ParagraphStyle\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Define a function to get synonyms\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wn.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())  # Add the synonyms to the set\n",
        "    return list(synonyms)\n",
        "\n",
        "# Create a mapping of values to related keywords/synonyms\n",
        "value_keywords = {\n",
        "    'inclusion': get_synonyms('inclusion'),\n",
        "    'learning': get_synonyms('learning'),\n",
        "    'safety': get_synonyms('safety'),\n",
        "    'integrity': get_synonyms('integrity')\n",
        "}\n",
        "\n",
        "# Define a function to check for values represented in sentences\n",
        "def values_in_sentence(sentence, value_keywords):\n",
        "    # Tokenize and tag parts of speech in the sentence\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "    # Check if sentence contains any form (synonyms, related words) of the values\n",
        "    for word, tag in tagged_words:\n",
        "        for value, synonyms in value_keywords.items():\n",
        "            if word.lower() in synonyms or word.lower() == value.lower():\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Define a function that utilizes the values to highlight sentences\n",
        "def custom_highlight_values_in_sentences(pdf_path, value_keywords, output_path):\n",
        "    doc = SimpleDocTemplate(output_path, pagesize=letter)\n",
        "    story = []\n",
        "\n",
        "    # Define custom paragraph styles for normal and highlighted text\n",
        "    normal_style = getSampleStyleSheet()['Normal']\n",
        "    highlight_style = ParagraphStyle(name='Highlight', parent=normal_style)\n",
        "    highlight_style.backColor = colors.yellow  # Set the background color\n",
        "\n",
        "    # Extract text and preprocess\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    sentences = preprocess_text(text)\n",
        "\n",
        "    # Append sentences to story with or without highlighting\n",
        "    for sentence in sentences:\n",
        "        if values_in_sentence(sentence, value_keywords):\n",
        "            p = Paragraph(sentence, highlight_style)\n",
        "        else:\n",
        "            p = Paragraph(sentence, normal_style)\n",
        "        story.append(p)\n",
        "        story.append(Spacer(1, 12))\n",
        "    # print(doc.build(story))\n",
        "    doc.build(story)\n",
        "\n",
        "# Process PDFs and highlight based on the new function\n",
        "# ... same directory setup ...\n",
        "\n",
        "# Iterate through PDF files and apply the new highlight function\n",
        "\n",
        "output_directory = '/content/drive/My Drive/Hacking/nltk-data'\n",
        "for pdf_filename in os.listdir(data_directory):\n",
        "    if pdf_filename.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(data_directory, pdf_filename)\n",
        "\n",
        "        # Highlight based on values and create a new PDF\n",
        "        highlighted_pdf_path = os.path.join(output_directory, \"highlighted_\" + pdf_filename)\n",
        "        custom_highlight_values_in_sentences(pdf_path, value_keywords, highlighted_pdf_path)\n",
        "\n",
        "        print(\"Highlighted PDF saved as:\", highlighted_pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdqy7K8ylM6g",
        "outputId": "70d492a7-8c60-4877-e4ec-5526a7411909"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highlighted PDF saved as: /content/drive/My Drive/Hacking/nltk-data/highlighted_akoehler-sop.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "# ... your previous imports ...\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Print output data for the specific PDF file\n",
        "target_pdf_filename = \"highlighted_akoehler-sop.pdf\"  # Replace with the actual filename\n",
        "\n",
        "output_directory = '/content/drive/My Drive/Hacking/nltk-data'\n",
        "\n",
        "target_pdf_path = os.path.join(output_directory, target_pdf_filename)\n",
        "\n",
        "if os.path.exists(target_pdf_path):\n",
        "    print(f\"Output data for {target_pdf_filename}:\\n\")\n",
        "\n",
        "    # Open and read the PDF file with PyMuPDF\n",
        "    pdf_document = fitz.open(target_pdf_path)\n",
        "\n",
        "    full_text = \"\"\n",
        "\n",
        "    for page_num in range(pdf_document.page_count):\n",
        "        page = pdf_document[page_num]\n",
        "\n",
        "        # Extract and print text from the page\n",
        "        page_text = page.get_text(\"text\")\n",
        "        full_text += page_text\n",
        "\n",
        "        # Extract and print highlighted data\n",
        "        for annot in page.annots():\n",
        "            if annot[\"subtype\"] == \"/Highlight\":\n",
        "                highlight_text = page.get_text(\"text\", clip=annot.rect)\n",
        "                full_text += f\"Highlighted Text: {highlight_text}\\n\"\n",
        "\n",
        "    print(full_text)\n",
        "    pdf_document.close()\n",
        "else:\n",
        "    print(f\"PDF '{target_pdf_filename}' not found in the output directory.\")\n"
      ],
      "metadata": {
        "id": "wpnfbxivtWiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f17fefb-420d-49d3-bbd8-d2089b8484d3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output data for highlighted_akoehler-sop.pdf:\n",
            "\n",
            "Adam Koehler Statement of Purpose After taking a multitude of computer science courses over my\n",
            "academic career I have perceived that most of the courses tend to build a student’s knowledge of\n",
            "comput er science outward, broadening familiarity over several areas of computer s cience rather than\n",
            "creating a concentrated depth of knowledge in any one topic.\n",
            "Pursuing a maste r’s degree in computer science will allow me to more fully develop my knowledg e in\n",
            "two areas of particular interest to me: artificial intelligence and c omputer graphics.\n",
            "By studying these areas in depth, I hope to resolve the disparity between ha ving a wide scope of\n",
            "computer science knowledge and having a more concentrated understanding of a single computer\n",
            "science field.\n",
            "Artificial intelligence is the foremost area of interest that I wish study further.\n",
            "The attraction of artificial intelligence for me lies in its breadt h of applicability, both as a method of\n",
            "problem solving in itself and in a symbiotic integration with other ar eas of computer science.\n",
            "A broad spectrum of applications exist within the artificial intelligence field, ranging from intelligent\n",
            "non-player controlled chara cters in computer game software to a ubiquitous computing solution that\n",
            "intelligently reacts to a variety of users.\n",
            "This diversity is one of the main reasons that I feel compelled to pur sue artificial intelligence further.\n",
            "While I have striven to develop my understanding of art ificial intelligence during my undergraduate\n",
            "education, the choreographed requireme nts of a bachelor's degree have restricted my research to only\n",
            "a minute sample o f artificial intelligence’s applications.\n",
            "During my exposure to the field, I have often b een unsatisfied with the level of interaction artificial\n",
            "intelligence displa ys in response to prompts of varying complexity.\n",
            "I do not believe the field has been develop ed to its potential in any regard, and feel that considerable\n",
            "progress can be made to i mprove the interactive experience that users have with an artificial intellig\n",
            "ence application.\n",
            "This genuine intrigue combined with my curiosity for the subject matter and the l imitless potential of the\n",
            "field are the reason why I wish to pursue a greater depth of knowledge in artificial intelligence.\n",
            "Through the education gained in pursuit of a master ’s degree, I hope to be able to enrich the\n",
            "authenticity of many artificial intell igence experiences, from computer games, to interactive toys and\n",
            "beyond.\n",
            "While artificial intelligence holds the most intrigue for me, a second ary area of interest is computer\n",
            "graphics.\n",
            "Akin to artificial intelligence in that I have only touched on the subject during my undergraduate career,\n",
            "I hope to explore the areas of two and three dimensional rendering more acutely.\n",
            "While I have always enjoyed the freedom of creative expression, and embrace its value in many\n",
            "aspects of probl em solving in computer science, the rigidity of programming has precluded my pursuit\n",
            "of many artistic interests.\n",
            "I feel that a more in-depth review of the field of computer gra phics would be very fulfilling for me, both\n",
            "for its creative liberties and for its appli cation to other areas of computer science.\n",
            "Specifically with regard to my interest in artif icial intelligence, I envision graphics applications such as\n",
            "an artificially intelligent avatar , with body and facial expressions that can create more engaging\n",
            "interaction betw een two users.\n",
            "It is also important to understand just how quickly the field of computer graphics is changing within\n",
            "computer science.\n",
            "I believe that a deeper appreciation for the s tate-of-the-art in computer graphics will only help me in all\n",
            "future computer science purs uits, and I look forward to the new challenges while pursuing my master’s\n",
            "degree in comp uter science.\n",
            "My undergraduate education has prepared me for the depth and commitment required of graduate\n",
            "research in the field of computer science.\n",
            "Two projects I have participated in would be research with the Embedded XINU team, and a senior\n",
            "design project also involving the XINU operating system.\n",
            "While participati ng on the Embedded XINU team I have chosen to aide in the research and\n",
            "development of an external a nalysis tool for embedded systems development.\n",
            "Personally, I have researched integr ating the front end system with the debugging system, developed\n",
            "by another team membe r, via an open source debugging software.\n",
            "Once complete, it will allow users to a ccess many debugging features, such as single stepping through\n",
            "code, breaking at a cer tain point in the code, and continuing the execution of the code from the\n",
            "breakpoint.\n",
            "The senior design project involves the creation of a software program to allow users bui lding a XINU\n",
            "operating system to directly use computers with a Windows operat ing system installed.\n",
            "Currently, a direct or remote connection to a UNIX variant needs t o be used to aide development.\n",
            "Both projects have allowed me to see the considerable amount of work and research that goes into\n",
            "software development projects.\n",
            "Each of the projects is team oriented; with tasks split down so that one or more researchers can contrib\n",
            "ute.\n",
            "The groups function with relative autonomy, without daily guidance of a professor , although guidance\n",
            "is provided on a semi-periodic basis.\n",
            "Overall, these projects have provided useful experiences allowing me to understand both graduate and\n",
            "undergraduate le vel research and implementation.\n",
            "In addition to these two XINU based projects, I have also completed sever al course-based projects\n",
            "that assimilate the entire semester's work i nto a cohesive project.\n",
            "One such project was a language interpreter written in Scheme that int erprets primitive operations\n",
            "including add, subtract, Boolean not, and, or, and xor; as well as complex statements such as variable\n",
            "assignment, print statements, procedure dec larations and calls and simple type checking.\n",
            "This and several other projects have helped me deve lop a wealth of knowledge in the computer\n",
            "science field, and have certainly broadened m y interests.\n",
            "However, they have lacked depth in certain areas that I feel tr uly drawn to exploring.\n",
            "Through graduate studies I hope to explore the fields of artificial intell igence and computer graphics\n",
            "more fully, and negotiate the requirements of intertwini ng the two more seamlessly than today's efforts\n",
            "produce.\n",
            "My ultimate goal is to produce meaningful work that combines the acquired in-depth knowledge of both\n",
            "the artificial intell igence field and the computer graphics field, while building on the solid foundation I\n",
            "have created in my undergraduate pursuits.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.platypus import Table, TableStyle\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access your files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize spaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ... Your other imports and code ...\n",
        "\n",
        "# Define a function to check for values represented in sentences using spaCy\n",
        "def values_in_sentence(sentence, value_keywords):\n",
        "    doc = nlp(sentence)\n",
        "    for token in doc:\n",
        "        for value, synonyms in value_keywords.items():\n",
        "            if token.text.lower() in synonyms or token.text.lower() == value.lower():\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# ... Your other functions ...\n",
        "\n",
        "# Define a function that utilizes the values to highlight sentences\n",
        "def custom_highlight_values_in_sentences(pdf_path, value_keywords, output_path):\n",
        "    doc = SimpleDocTemplate(output_path, pagesize=letter)\n",
        "    story = []\n",
        "\n",
        "    # Define custom paragraph styles for normal and highlighted text\n",
        "    normal_style = getSampleStyleSheet()['Normal']\n",
        "    highlight_style = ParagraphStyle(name='Highlight', parent=normal_style)\n",
        "    highlight_style.backColor = colors.yellow  # Set the background color\n",
        "\n",
        "    # Extract text and preprocess\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    sentences = preprocess_text(text)\n",
        "\n",
        "    # Append sentences to story with or without highlighting\n",
        "    for sentence in sentences:\n",
        "        if values_in_sentence(sentence, value_keywords):\n",
        "            p = Paragraph(sentence, highlight_style)\n",
        "        else:\n",
        "            p = Paragraph(sentence, normal_style)\n",
        "        story.append(p)\n",
        "        story.append(Spacer(1, 12))\n",
        "\n",
        "    doc.build(story)\n",
        "\n",
        "# Process PDFs and highlight based on the new function\n",
        "# ... Same directory setup ...\n",
        "\n",
        "# Iterate through PDF files and apply the new highlight function\n",
        "output_directory = '/content/drive/My Drive/Hacking/nltk-data2'\n",
        "for pdf_filename in os.listdir(data_directory):\n",
        "    if pdf_filename.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(data_directory, pdf_filename)\n",
        "\n",
        "        # Highlight based on values and create a new PDF\n",
        "        highlighted_pdf_path = os.path.join(output_directory, \"highlighted_\" + pdf_filename)\n",
        "        custom_highlight_values_in_sentences(pdf_path, value_keywords, highlighted_pdf_path)\n",
        "\n",
        "        print(\"Highlighted PDF saved as:\", highlighted_pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjnGyVk9tWlS",
        "outputId": "8cfdd6f4-8b88-4c2d-9cc3-f1e1c4821b80"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Highlighted PDF saved as: /content/drive/My Drive/Hacking/nltk-data2/highlighted_akoehler-sop.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3gUZKVGXvB56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... your imports ...\n",
        "\n",
        "# Initialize spaCy NLP model with custom NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Add custom NER labels for UB values (e.g., \"University at Buffalo,\" \"excellence\")\n",
        "for value in value_keywords:\n",
        "    nlp.vocab.strings.add(value)\n",
        "\n",
        "# Define a function for custom NER\n",
        "def custom_ner(doc):\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in value_keywords:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Define a function to check for values represented in sentences using spaCy\n",
        "def values_in_sentence(sentence, value_keywords):\n",
        "    doc = nlp(sentence)\n",
        "    if custom_ner(doc):\n",
        "        return True\n",
        "    for token in doc:\n",
        "        for value, synonyms in value_keywords.items():\n",
        "            if token.text.lower() in synonyms or token.text.lower() == value.lower():\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# ... your other functions ...\n",
        "\n",
        "# Iterate through PDF files and apply the new highlight function\n",
        "output_directory = '/content/drive/My Drive/Hacking/nltk-data3'\n",
        "for pdf_filename in os.listdir(data_directory):\n",
        "    if pdf_filename.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(data_directory, pdf_filename)\n",
        "\n",
        "        # Highlight based on values and create a new PDF\n",
        "        highlighted_pdf_path = os.path.join(output_directory, \"highlighted_\" + pdf_filename)\n",
        "        custom_highlight_values_in_sentences(pdf_path, value_keywords, highlighted_pdf_path)\n",
        "\n",
        "        print(\"Highlighted PDF saved as:\", highlighted_pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkAVDrhDvB83",
        "outputId": "1b4d5a6f-6fa6-4611-bf0e-4ac05d37e491"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highlighted PDF saved as: /content/drive/My Drive/Hacking/nltk-data3/highlighted_akoehler-sop.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PqQbz8DewNi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.platypus import Table, TableStyle\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from google.colab import drive\n",
        "import re\n",
        "\n",
        "# Mount Google Drive to access your files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize spaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ... Your other imports and code ...\n",
        "\n",
        "# Define a function to check for values represented in sentences using spaCy\n",
        "def values_in_sentence(sentence, value_keywords):\n",
        "    doc = nlp(sentence)\n",
        "    for token in doc:\n",
        "        for value, synonyms in value_keywords.items():\n",
        "            if token.text.lower() in synonyms or token.text.lower() == value.lower():\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# ... Your other functions ...\n",
        "\n",
        "# Define a function to convert the text to HTML with highlighted sentences\n",
        "def text_to_html_with_highlights(text, value_keywords):\n",
        "    # Split the text into sentences\n",
        "    sentences = preprocess_text(text)\n",
        "\n",
        "    # Create an HTML string with <p> tags\n",
        "    html_text = \"<p>\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if values_in_sentence(sentence, value_keywords):\n",
        "            html_text += f'<span style=\"background-color: yellow;\">{sentence}</span> '\n",
        "        else:\n",
        "            html_text += f'{sentence} '\n",
        "\n",
        "    html_text += \"</p>\"\n",
        "\n",
        "    return html_text\n",
        "\n",
        "# Process PDFs and generate HTML with highlights\n",
        "# ... Same directory setup ...\n",
        "\n",
        "# Iterate through PDF files and apply the new function\n",
        "\n",
        "output_directory = '/content/drive/My Drive/Hacking/htmls'\n",
        "for pdf_filename in os.listdir(data_directory):\n",
        "    if pdf_filename.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(data_directory, pdf_filename)\n",
        "\n",
        "        # Extract text from the PDF\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Generate HTML with highlights\n",
        "        html_with_highlights = text_to_html_with_highlights(text, value_keywords)\n",
        "\n",
        "        # Save the HTML to a file or use it as needed\n",
        "        html_output_path = os.path.join(output_directory, \"highlighted_\" + os.path.splitext(pdf_filename)[0] + \".html\")\n",
        "\n",
        "        with open(html_output_path, \"w\", encoding=\"utf-8\") as html_file:\n",
        "            html_file.write(html_with_highlights)\n",
        "\n",
        "        print(\"HTML with highlights saved as:\", html_output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEf2wc_CwNl7",
        "outputId": "9ab073c7-e88b-4d0c-f76b-dcbb54bf15fd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/htmls/highlighted_akoehler-sop.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FzxcEUkq6p9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7Slle_E6qE1",
        "outputId": "946eeca6-224b-4612-d34d-f34eb1b76de1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Collecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=50706763dc1e83d0374b1030c8a4c2d3d248ee4f954f4a36022061c6d221c93a\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spacy - final model**"
      ],
      "metadata": {
        "id": "KGXlFmC6IDv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.platypus import Table, TableStyle\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access your files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize spaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the UB values\n",
        "ub_values = [\"inclusion\", \"learning\", \"safety\", \"integrity\"]\n",
        "\n",
        "# Define a function to identify UB values in a sentence\n",
        "def identify_ub_values(sentence, ub_values):\n",
        "    doc = nlp(sentence)\n",
        "    for token in doc:\n",
        "        for value in ub_values:\n",
        "            similarity = token.similarity(nlp(value))\n",
        "            if similarity > 0.6:  # Adjust the similarity threshold as needed\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# Define a function to convert the text to HTML with highlighted sentences\n",
        "def text_to_html_with_highlights(text, ub_values):\n",
        "    # Split the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Create an HTML string with <p> tags\n",
        "    html_text = \"<p>\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if identify_ub_values(sentence, ub_values):\n",
        "            html_text += f'<span style=\"background-color: yellow;\">{sentence}</span> '\n",
        "        else:\n",
        "            html_text += f'{sentence} '\n",
        "\n",
        "    html_text += \"</p>\"\n",
        "\n",
        "    return html_text\n",
        "\n",
        "# Process PDFs and generate HTML with highlights\n",
        "# ... Same directory setup ...\n",
        "\n",
        "# Iterate through PDF files and apply the new function\n",
        "\n",
        "output_directory = '/content/drive/My Drive/Hacking/spacy-html'\n",
        "for pdf_filename in os.listdir(data_directory):\n",
        "    if pdf_filename.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(data_directory, pdf_filename)\n",
        "\n",
        "        # Extract text from the PDF\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Generate HTML with highlights\n",
        "        html_with_highlights = text_to_html_with_highlights(text, ub_values)\n",
        "\n",
        "        # Save the HTML to a file or use it as needed\n",
        "        html_output_path = os.path.join(output_directory, \"highlighted_\" + os.path.splitext(pdf_filename)[0] + \".html\")\n",
        "\n",
        "        with open(html_output_path, \"w\", encoding=\"utf-8\") as html_file:\n",
        "            html_file.write(html_with_highlights)\n",
        "\n",
        "        print(\"HTML with highlights saved as:\", html_output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ohcXFue6tbz",
        "outputId": "fb7eb904-c6db-42ba-a386-bb0dee2c1f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-142-ddae89e4f0b6>:27: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity = token.similarity(nlp(value))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_AARESH_SOP2.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_Siddharth_Sharma_SOP.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_PRAMOD SOP ECE.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_Siddharth_Sharma_SOP (1).html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_SOP 6-CS.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_SOP 2-CS.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_samplesop ECE.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_Final Reviewed SOP-UK.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_STATEMENT OF PURPOS2 ECE.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_SOP_MSOR_Columbia.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_STATEMENT OF PURPOS1 ECE.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_Statement of Purpose1 A GUIDE_1.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_SOP_MSMIS_UIC.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_STATEMENT OF PURPOSE IND ENG.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_SOP_MSIS_KSB.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_SOP_MEngEM_Cornell.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_akoehler-sop.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_sop mi.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_Satish BS_SOP_MEM.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_Satish BS_SOP_MSIS.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_SOP_MEM_Dartmouth.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_SOP_MSEM_UCI.doc.html\n",
            "HTML with highlights saved as: /content/drive/My Drive/Hacking/spacy-html/highlighted_SOP_MEM_Duke.doc.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Define the path to the HTML file\n",
        "html_file_path = \"/content/drive/My Drive/Hacking/spacy-html/highlighted_akoehler-sop.html\"\n",
        "\n",
        "# Read the HTML content from the file\n",
        "with open(html_file_path, \"r\", encoding=\"utf-8\") as html_file:\n",
        "    html_content = html_file.read()\n",
        "\n",
        "# Display the HTML content with highlighted sentences\n",
        "display(HTML(html_content))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "lBnxeqPm8BDV",
        "outputId": "f93b0d47-fd90-4599-f612-543bdd0dc50c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p><span style=\"background-color: yellow;\">Adam Koehler \n",
              "Statement of Purpose \n",
              " \n",
              " After taking a multitude of computer science courses over my academic career I \n",
              "have perceived that most of the courses tend to build a student’s knowledge of comput er \n",
              "science outward, broadening familiarity over several areas of computer s cience rather \n",
              "than creating a concentrated depth of knowledge in any one topic.</span> <span style=\"background-color: yellow;\">Pursuing a maste r’s \n",
              "degree in computer science will allow me to more fully develop my knowledg e in two \n",
              "areas of particular interest to me: artificial intelligence and c omputer graphics.</span> <span style=\"background-color: yellow;\">By \n",
              "studying these areas in depth, I hope to resolve the disparity between ha ving a wide scope \n",
              "of computer science knowledge and having a more concentrated understanding of a \n",
              "single computer science field.</span> Artificial intelligence is the foremost area of interest that I wish study further. <span style=\"background-color: yellow;\">The attraction of artificial intelligence for me lies in its breadt h of applicability, both as a \n",
              "method of problem solving in itself and in a symbiotic integration with other ar eas of \n",
              "computer science.</span> A broad spectrum of applications exist within the  artificial \n",
              "intelligence field, ranging from intelligent non-player controlled chara cters in computer \n",
              "game software to a ubiquitous computing solution that intelligently reacts  to a variety of \n",
              "users. This diversity is one of the main reasons that I feel compelled to pur sue artificial \n",
              "intelligence further. While I have striven to develop my understanding of art ificial \n",
              "intelligence during my undergraduate education, the choreographed requireme nts of a \n",
              "bachelor's degree have restricted my research to only a minute sample o f artificial \n",
              "intelligence’s applications. During my exposure to the field, I have often b een \n",
              "unsatisfied with the level of interaction artificial intelligence displa ys in response to \n",
              "prompts of varying complexity. <span style=\"background-color: yellow;\">I do not believe the field has been develop ed to its \n",
              "potential in any regard, and feel that considerable progress can be made to i mprove the \n",
              "interactive experience that users have with an artificial intellig ence application.</span> <span style=\"background-color: yellow;\">This \n",
              "genuine intrigue combined with my curiosity for the subject matter and the l imitless \n",
              "potential of the field are the reason why I wish to pursue a greater depth of  knowledge in \n",
              "artificial intelligence.</span> Through the education gained in pursuit of a master ’s degree, I \n",
              "hope to be able to enrich the authenticity of many artificial intell igence experiences, from \n",
              "computer games, to interactive toys and beyond. While artificial intelligence holds the most intrigue for me, a second ary area of \n",
              "interest is computer graphics. Akin to artificial intelligence in that I have only touched \n",
              "on the subject during my undergraduate career, I hope to explore the areas of two and \n",
              "three dimensional rendering more acutely. <span style=\"background-color: yellow;\">While I have always enjoyed the  freedom of \n",
              "creative expression, and embrace its value in many aspects of probl em solving in \n",
              "computer science, the rigidity of programming has precluded my pursuit  of many artistic \n",
              "interests.</span> I feel that a more in-depth review of the field of computer gra phics would be \n",
              "very fulfilling for me, both for its creative liberties and for its appli cation to other areas of \n",
              "computer science. Specifically with regard to my interest in artif icial intelligence, I \n",
              "envision graphics applications such as an artificially intelligent avatar , with body and \n",
              "facial expressions that can create more engaging interaction betw een two users. It is also \n",
              "important to understand just how quickly the field of computer graphics is changing  \n",
              "within computer science. I believe that a deeper appreciation for the s tate-of-the-art in computer graphics will only help me in all future computer science purs uits, and I look \n",
              "forward to the new challenges while pursuing my master’s degree in comp uter science. My undergraduate education has prepared me for the depth and commitment \n",
              "required of graduate research in the field of computer science. Two projects  I have \n",
              "participated in would be research with the Embedded XINU team, and a senior  design \n",
              "project also involving the XINU operating system. While participati ng on the Embedded \n",
              "XINU team I have chosen to aide in the research and development of an external a nalysis \n",
              "tool for embedded systems development. <span style=\"background-color: yellow;\">Personally, I have researched integr ating the \n",
              "front end system with the debugging system, developed by another team membe r, via an \n",
              "open source debugging software.</span> <span style=\"background-color: yellow;\">Once complete, it will allow users to a ccess many \n",
              "debugging features, such as single stepping through code, breaking at a cer tain point in \n",
              "the code, and continuing the execution of the code from the breakpoint.</span> <span style=\"background-color: yellow;\">The senior \n",
              "design project involves the creation of a software program to allow users bui lding a \n",
              "XINU operating system to directly use computers with a Windows operat ing system \n",
              "installed.</span> Currently, a direct or remote connection to a UNIX variant needs t o be used to \n",
              "aide development. Both projects have allowed me to see the considerable amount  of \n",
              "work and research that goes into software development projects. Each of the  projects is \n",
              "team oriented; with tasks split down so that one or more researchers can contrib ute. The \n",
              "groups function with relative autonomy, without daily guidance of a professor , although \n",
              "guidance is provided on a semi-periodic basis. Overall, these projects  have provided \n",
              "useful experiences allowing me to understand both graduate and undergraduate le vel \n",
              "research and implementation. <span style=\"background-color: yellow;\">In addition to these two XINU based projects, I have also completed sever al \n",
              "course-based projects that assimilate the entire semester's work i nto a cohesive project.</span> One such project was a language interpreter written in Scheme that int erprets primitive \n",
              "operations including add, subtract, Boolean not, and, or, and xor; as well as complex  \n",
              "statements such as variable assignment, print statements, procedure dec larations and calls \n",
              "and simple type checking. This and several other projects have helped me deve lop a \n",
              "wealth of knowledge in the computer science field, and have certainly broadened m y \n",
              "interests. <span style=\"background-color: yellow;\">However, they have lacked depth in certain areas that I feel tr uly drawn to \n",
              "exploring.</span> Through graduate studies I hope to explore the fields of artificial intell igence and \n",
              "computer graphics more fully, and negotiate the requirements of intertwini ng the two \n",
              "more seamlessly than today's efforts produce. My ultimate goal is to produce meaningful \n",
              "work that combines the acquired in-depth knowledge of both the artificial intell igence \n",
              "field and the computer graphics field, while building on the solid foundation I have \n",
              "created in my undergraduate pursuits. </p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hxjTjx-2AUI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rating SOP as per UB and Department Values**"
      ],
      "metadata": {
        "id": "vQPoyRen9wfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import spacy\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access your files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize spaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the UB values and CSE department values\n",
        "ub_values = [\"inclusion\", \"learning\", \"safety\", \"integrity\"]\n",
        "cse_values = [\"diversity\", \"equity\", \"inclusiveness\", \"integrity\"]\n",
        "\n",
        "# Define a function to calculate the relevance score for a sentence and values\n",
        "def calculate_sentence_relevance(sentence, values):\n",
        "    doc = nlp(sentence)\n",
        "    relevance_scores = {value: 0 for value in values}\n",
        "\n",
        "    for token in doc:\n",
        "        for value in values:\n",
        "            similarity = token.similarity(nlp(value))\n",
        "            relevance_scores[value] += similarity\n",
        "\n",
        "    return relevance_scores\n",
        "\n",
        "# Define a function to calculate the ratings for all values\n",
        "def calculate_ratings(text, ub_values, cse_values):\n",
        "    ub_ratings = calculate_sentence_relevance(text, ub_values)\n",
        "    cse_ratings = calculate_sentence_relevance(text, cse_values)\n",
        "\n",
        "    max_ub_rating = max(ub_ratings.values())\n",
        "    max_cse_rating = max(cse_ratings.values())\n",
        "\n",
        "    if max_ub_rating > 0:\n",
        "        ub_ratings = {value: round((rating / max_ub_rating) * 5) for value, rating in ub_ratings.items()}\n",
        "    if max_cse_rating > 0:\n",
        "        cse_ratings = {value: round((rating / max_cse_rating) * 5) for value, rating in cse_ratings.items()}\n",
        "\n",
        "    return ub_ratings, cse_ratings\n",
        "\n",
        "# Process PDFs and calculate ratings for all values\n",
        "data_directory = '/content/drive/My Drive/Hacking/Data'  # Update with your SOPs directory\n",
        "output_directory = '/content/drive/My Drive/Hacking/spacy-html-with-ratings'\n",
        "\n",
        "for pdf_filename in os.listdir(data_directory):\n",
        "    if pdf_filename.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(data_directory, pdf_filename)\n",
        "\n",
        "        # Extract text from the PDF\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Calculate ratings for all values based on relevance\n",
        "        ub_ratings, cse_ratings = calculate_ratings(text, ub_values, cse_values)\n",
        "\n",
        "        # Save the HTML with relevance ratings to a file\n",
        "        html_output_path = os.path.join(output_directory, \"highlighted_\" + os.path.splitext(pdf_filename)[0] + \".html\")\n",
        "        with open(html_output_path, \"w\", encoding=\"utf-8\") as html_file:\n",
        "            html_file.write(f\"<p><b>UB Value Ratings:</b> {ub_ratings}</p>\")\n",
        "            html_file.write(f\"<p><b>CSE Department Ratings:</b> {cse_ratings}</p>\")\n",
        "\n",
        "        print(\"HTML with ratings saved as:\", html_output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj-3_Q3FIHkq",
        "outputId": "670c2d81-c198-4453-c734-593440a3fe31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-148-eeb68915f6bb>:24: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity = token.similarity(nlp(value))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_AARESH_SOP2.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_Siddharth_Sharma_SOP.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_PRAMOD SOP ECE.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_Siddharth_Sharma_SOP (1).html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_SOP 6-CS.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_SOP 2-CS.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_samplesop ECE.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_Final Reviewed SOP-UK.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_STATEMENT OF PURPOS2 ECE.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_SOP_MSOR_Columbia.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_STATEMENT OF PURPOS1 ECE.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_Statement of Purpose1 A GUIDE_1.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_SOP_MSMIS_UIC.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_STATEMENT OF PURPOSE IND ENG.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_SOP_MSIS_KSB.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_SOP_MEngEM_Cornell.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_akoehler-sop.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_sop mi.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_Satish BS_SOP_MEM.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_Satish BS_SOP_MSIS.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_SOP_MEM_Dartmouth.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_SOP_MSEM_UCI.doc.html\n",
            "HTML with ratings saved as: /content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_SOP_MEM_Duke.doc.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Define the path to the HTML file\n",
        "html_file_path = \"/content/drive/My Drive/Hacking/spacy-html-with-ratings/highlighted_akoehler-sop.html\"\n",
        "\n",
        "# Read the HTML content from the file\n",
        "with open(html_file_path, \"r\", encoding=\"utf-8\") as html_file:\n",
        "    html_content = html_file.read()\n",
        "\n",
        "# Display the HTML content with ratings\n",
        "display(HTML(html_content))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "F8Iw2Usp8FqB",
        "outputId": "997100be-7282-486c-d975-6fa7c67dee6d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p><b>UB Value Ratings:</b> {'inclusion': 5, 'learning': 2, 'safety': 5, 'integrity': 5}</p><p><b>CSE Department Ratings:</b> {'diversity': 4, 'equity': 4, 'inclusiveness': 4, 'integrity': 5}</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8wwD35-u8SPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fetching Achievements from Resumes**"
      ],
      "metadata": {
        "id": "zYbrPCsI8x3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_line_breaks_after_fullstops(input_string):\n",
        "    # Replace full stops with full stop followed by a line break\n",
        "    modified_string = input_string.replace(\".\", \".\\n\")\n",
        "\n",
        "\n",
        "    return modified_string\n",
        "\n",
        "# Example usage:\n",
        "original_string = \"(716) 159 5818 • johnpescud@gmail.com ____________________________________________________________________________________________________________ EDUCATION Master of Science – Computer Science and Engineering GPA 3.83 out of 4 The State University of New York at Buffalo Jan 2024 Bachelor of Technology – Computer Science and Engineering GPA 8.49 out of 10 VIT University, Vellore Institute of Technology June 2020 SKILLS Programming Languages – Java, Python, TypeScript, JavaScript. Backend Frameworks – Spring Boot, Hibernate, Node JS. Frontend Frameworks – Native Android App Development, Angular 14, React JS. Cloud technologies – AWS EC2, Elastic Beanstalk, EC2, S3, Lambda. Tools – Docker, Apache Kafka, Rabbit MQ, Jenkins, SonarQube, Redis, Git, Netflix OSS, Zuul, SQL, MongoDB, CheckMarx, Vault.., writing strategy status small recommendations quantitative problem solving skills metrics li growth feedback execution continuous improvement communicvision strategic solute unit test p Jenkins, Sonar Cube, CheckMarx, Fossa, and GitHub Refactor API, SpringBoot/Java API/microservices , user stories user experience tdd swagger status solutions software engineer software developer software rest problem solving kanban json issues interpersonal skills gcp full stack software engineer decision making cookies continuous integration continuous deployment collaborative best practices websockets startup software engineer software development software developer software node.js kubernetes infrastructure graphql fintech data structure cross functionally consumer communication skills owershell bash testing software developer rhel operating systems labview embedded systems confluence centos bitbucket testing svn ssi problem solving networking mercurial collaboration ons visio vision testing software regression programming language multitask legal http groovy fast paced environment ethernet documentation continuous improvement communication skills ci/cd basic analytical skills n vision strategic status solutions software security infrastructure consumer status sql server solutions software development software self starter sales problem solving performance issues information system devops database administration communication skills collaborative cloud infrastructure basic azure application development strategic status solutions software security infrastructure entrepreneur consumer analytical and problemsoftware security powerpoint infrastructure go excel analytical and problemate change software development information system consulting computer engineer collaborativemanagement analytical web application development version control testing specification software development software schema rest relational database react native react rds project management php lamp jquery issues front end development fast paced ec2 drupal dom design patterns deployment continuous integration compliance communication artificial intelligence application developer application api ajax agile methodologie software self relationship management problem solving mobile app market issues information technology finance decision making critical thinking communication coding circle charity c analytics ai Troubleshooting, teamwork, software, revolution, issues, hardware, debugging, cleansoftware engineer software open source innovation html5 data sets cross functional collaboration continuous learning communication collaborative c++ c# big data technologies big data back end xpath xml written wireframe web technologies web services web application vuejs visual design verbal communication skills validation ux user stories user interface user experience usability ui/ux ui developer ui uat testing test plan svn strategic statistics startup stakeholders sr sqlite spring mvc software development software developer software soap small ship self starter self motivate selenium security sdk scrum scripting scipy scalable sales pipeline sales saas roadmap restful rest reliability release management release regression rapid prototyping r&d r qa product management product features product design product problem solving skills predictive model pipeline photoshop optimization objective c numpy nosql multitask ml model microsoft office maven machine learning linux latency jsp json jira issues interface innovative infrastructure illustrator ide html5 gtm flow fast paced entrepreneurial early stage startups e commerce detail oriented design review design patterns dataset data structure data science data modeling customer oriented css3 cross functional crm continuous learning consultative collaborate code review cocoa business objectives business intelligence business analysis bootstrap best practices basic automation tools advanced analytics acquisition TECHNICAL EXPERIENCES Google, California, Machine Learning Engineer Intern Aug 2023 – present • Developed NLP Model to detect Racial discrimination from video dataset, collaborated with a Realtime pipeline for detection. Amazon Intern, Research Assistant, Buffalo NY, Cloud Developer Jan 2023 – May 2023 • Collaborated at Transfer Scheduler, facilitating transfer of terabytes of data from Google Drive, ftp using Kafka and RabbitMQ. • Infrastructure setup – Implemented PKI secrets engine using Vault, enabling token management with fine grained ACL policies. Tata Consultancy Services TCS Digital, Bangalore, India, Cloud Engineer Aug 2020 – July 2022 • Led the design and development of Western Union's money transfer solution using Spring Java Microservice, collaborating with 10+ teams and re-architecting the WillCallStore (WCSTOX) program's solution to manage transactions from 50+ countries. • Developed a microservice layer that validates and stores all money order records in postgres and connecting data-stream Kafka, configurations of multiple Kafka topics, devised a common encryption layer to ensure the security of sensitive data. • Built and trained an NLP model to optimize code conversion from one syntax to another, identifying and resolving ambiguities. Achieved an impressive accuracy of 88% for converting Cobol to Java (Spring) for a codebase exceeding 10 million lines. • Led a team of 10 to provide efficient troubleshooting capabilities, designed, & built Angular webapp- searching Money records. TCS Digital, Bangalore, India, Cloud Engineer Intern Jan 2020 – April 2020 • Collaborated at CMA (Cloud Microservices Amplification) Department to support the development of a large scaled solution for a MNC, incorporating server side load balancing -Netflix OSS, Ribbon and Jenkins pipeline, deployment on AWS. PROJECTS • OpenAI API (2023), Personal Project – Designed Maven and Gradle build tools for OpenAI API, enabling asynchronous calling of all endpoints. Implemented multithreading support for enhanced performance and efficiency in API usage. • Dall E 2 Client (2023), Personal Project – An open-sourced Android App that allows content creators to generate images using OpenAI’s – Dall–E API and post on social media, developed within 2 days following the launch of APIs. • Click-Bait Spoiler Detection (2023), School of computer science – Achieved 73% accuracy by utilizing advanced algorithms (RoBERTa, DistillBERT, Sentence Transformers) to classify and detect clickbait spoilers in various text formats. • Car Agent reward-based reinforcement learning project (2023) School of computer science – Implemented a graphical grid world with different reward based algorithms – greedy, SARSA, Q-learning using different strategies – stochastic, deterministic. • AR–Store (2023), School of computer science – App-visualizing various scientific concepts in AR using Unity. • Coneato (2022), Personal Project – A Hyper Casual Multiplayer Machine Learning Game for Android. RESEARCH PAPERS/ EXPERIENCE • Lead Author/ Springer Singapore Journal – 2020 MARCH, Molecular Dynamics using OpenMP+ SIMD. • Lead Author/ IEEE Vizag Bay Section – 2021 MARCH, Analysis of Machine Lexical using NLP. ACTIVITIES & ACHIEVEMENTS • Mentor at Grace Hopper Conference 2023. 2023 • Administered workshop on Git and GitHub for Master & PhD students, under guidance of Alina Vereshchaka. 2023 • Special Achievement Award by TCS – Employee of week during award Sprint. 2021 • Lead Android Developer and Board of Management at Apple Developers Group – Vellore, India 2020 • 1st Prize at MLH Local – Hackday – invented a human sentiment analysis using classification – App. 2019 • 3rd Prize Winner at App Challenge – tech fest of VIT, Launched an ecommerce app in less than 12 hrs. 2019\"\n",
        "\n",
        "result = add_line_breaks_after_fullstops(original_string)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAKZx9GN8SEB",
        "outputId": "e84ad1a8-436f-4616-eaea-318d45276a59"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(716) 159 5818 • johnpescud@gmail.\n",
            "com ____________________________________________________________________________________________________________ EDUCATION Master of Science – Computer Science and Engineering GPA 3.\n",
            "83 out of 4 The State University of New York at Buffalo Jan 2024 Bachelor of Technology – Computer Science and Engineering GPA 8.\n",
            "49 out of 10 VIT University, Vellore Institute of Technology June 2020 SKILLS Programming Languages – Java, Python, TypeScript, JavaScript.\n",
            " Backend Frameworks – Spring Boot, Hibernate, Node JS.\n",
            " Frontend Frameworks – Native Android App Development, Angular 14, React JS.\n",
            " Cloud technologies – AWS EC2, Elastic Beanstalk, EC2, S3, Lambda.\n",
            " Tools – Docker, Apache Kafka, Rabbit MQ, Jenkins, SonarQube, Redis, Git, Netflix OSS, Zuul, SQL, MongoDB, CheckMarx, Vault.\n",
            ".\n",
            ", writing strategy status small recommendations quantitative problem solving skills metrics li growth feedback execution continuous improvement communicvision strategic solute unit test p Jenkins, Sonar Cube, CheckMarx, Fossa, and GitHub Refactor API, SpringBoot/Java API/microservices , user stories user experience tdd swagger status solutions software engineer software developer software rest problem solving kanban json issues interpersonal skills gcp full stack software engineer decision making cookies continuous integration continuous deployment collaborative best practices websockets startup software engineer software development software developer software node.\n",
            "js kubernetes infrastructure graphql fintech data structure cross functionally consumer communication skills owershell bash testing software developer rhel operating systems labview embedded systems confluence centos bitbucket testing svn ssi problem solving networking mercurial collaboration ons visio vision testing software regression programming language multitask legal http groovy fast paced environment ethernet documentation continuous improvement communication skills ci/cd basic analytical skills n vision strategic status solutions software security infrastructure consumer status sql server solutions software development software self starter sales problem solving performance issues information system devops database administration communication skills collaborative cloud infrastructure basic azure application development strategic status solutions software security infrastructure entrepreneur consumer analytical and problemsoftware security powerpoint infrastructure go excel analytical and problemate change software development information system consulting computer engineer collaborativemanagement analytical web application development version control testing specification software development software schema rest relational database react native react rds project management php lamp jquery issues front end development fast paced ec2 drupal dom design patterns deployment continuous integration compliance communication artificial intelligence application developer application api ajax agile methodologie software self relationship management problem solving mobile app market issues information technology finance decision making critical thinking communication coding circle charity c analytics ai Troubleshooting, teamwork, software, revolution, issues, hardware, debugging, cleansoftware engineer software open source innovation html5 data sets cross functional collaboration continuous learning communication collaborative c++ c# big data technologies big data back end xpath xml written wireframe web technologies web services web application vuejs visual design verbal communication skills validation ux user stories user interface user experience usability ui/ux ui developer ui uat testing test plan svn strategic statistics startup stakeholders sr sqlite spring mvc software development software developer software soap small ship self starter self motivate selenium security sdk scrum scripting scipy scalable sales pipeline sales saas roadmap restful rest reliability release management release regression rapid prototyping r&d r qa product management product features product design product problem solving skills predictive model pipeline photoshop optimization objective c numpy nosql multitask ml model microsoft office maven machine learning linux latency jsp json jira issues interface innovative infrastructure illustrator ide html5 gtm flow fast paced entrepreneurial early stage startups e commerce detail oriented design review design patterns dataset data structure data science data modeling customer oriented css3 cross functional crm continuous learning consultative collaborate code review cocoa business objectives business intelligence business analysis bootstrap best practices basic automation tools advanced analytics acquisition TECHNICAL EXPERIENCES Google, California, Machine Learning Engineer Intern Aug 2023 – present • Developed NLP Model to detect Racial discrimination from video dataset, collaborated with a Realtime pipeline for detection.\n",
            " Amazon Intern, Research Assistant, Buffalo NY, Cloud Developer Jan 2023 – May 2023 • Collaborated at Transfer Scheduler, facilitating transfer of terabytes of data from Google Drive, ftp using Kafka and RabbitMQ.\n",
            " • Infrastructure setup – Implemented PKI secrets engine using Vault, enabling token management with fine grained ACL policies.\n",
            " Tata Consultancy Services TCS Digital, Bangalore, India, Cloud Engineer Aug 2020 – July 2022 • Led the design and development of Western Union's money transfer solution using Spring Java Microservice, collaborating with 10+ teams and re-architecting the WillCallStore (WCSTOX) program's solution to manage transactions from 50+ countries.\n",
            " • Developed a microservice layer that validates and stores all money order records in postgres and connecting data-stream Kafka, configurations of multiple Kafka topics, devised a common encryption layer to ensure the security of sensitive data.\n",
            " • Built and trained an NLP model to optimize code conversion from one syntax to another, identifying and resolving ambiguities.\n",
            " Achieved an impressive accuracy of 88% for converting Cobol to Java (Spring) for a codebase exceeding 10 million lines.\n",
            " • Led a team of 10 to provide efficient troubleshooting capabilities, designed, & built Angular webapp- searching Money records.\n",
            " TCS Digital, Bangalore, India, Cloud Engineer Intern Jan 2020 – April 2020 • Collaborated at CMA (Cloud Microservices Amplification) Department to support the development of a large scaled solution for a MNC, incorporating server side load balancing -Netflix OSS, Ribbon and Jenkins pipeline, deployment on AWS.\n",
            " PROJECTS • OpenAI API (2023), Personal Project – Designed Maven and Gradle build tools for OpenAI API, enabling asynchronous calling of all endpoints.\n",
            " Implemented multithreading support for enhanced performance and efficiency in API usage.\n",
            " • Dall E 2 Client (2023), Personal Project – An open-sourced Android App that allows content creators to generate images using OpenAI’s – Dall–E API and post on social media, developed within 2 days following the launch of APIs.\n",
            " • Click-Bait Spoiler Detection (2023), School of computer science – Achieved 73% accuracy by utilizing advanced algorithms (RoBERTa, DistillBERT, Sentence Transformers) to classify and detect clickbait spoilers in various text formats.\n",
            " • Car Agent reward-based reinforcement learning project (2023) School of computer science – Implemented a graphical grid world with different reward based algorithms – greedy, SARSA, Q-learning using different strategies – stochastic, deterministic.\n",
            " • AR–Store (2023), School of computer science – App-visualizing various scientific concepts in AR using Unity.\n",
            " • Coneato (2022), Personal Project – A Hyper Casual Multiplayer Machine Learning Game for Android.\n",
            " RESEARCH PAPERS/ EXPERIENCE • Lead Author/ Springer Singapore Journal – 2020 MARCH, Molecular Dynamics using OpenMP+ SIMD.\n",
            " • Lead Author/ IEEE Vizag Bay Section – 2021 MARCH, Analysis of Machine Lexical using NLP.\n",
            " ACTIVITIES & ACHIEVEMENTS • Mentor at Grace Hopper Conference 2023.\n",
            " 2023 • Administered workshop on Git and GitHub for Master & PhD students, under guidance of Alina Vereshchaka.\n",
            " 2023 • Special Achievement Award by TCS – Employee of week during award Sprint.\n",
            " 2021 • Lead Android Developer and Board of Management at Apple Developers Group – Vellore, India 2020 • 1st Prize at MLH Local – Hackday – invented a human sentiment analysis using classification – App.\n",
            " 2019 • 3rd Prize Winner at App Challenge – tech fest of VIT, Launched an ecommerce app in less than 12 hrs.\n",
            " 2019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Sample resume content (replace this with the actual content)\n",
        "oldresume_content = \"\"\"\n",
        "Experienced software engineer with a strong background in developing scalable data pipelines.\n",
        "Awarded 'Employee of the Year' for exceptional performance and dedication.\n",
        "Implemented a new caching algorithm that improved system efficiency by 30%.\n",
        "Lead a team that developed an app winning 'Best Innovation' at the XYZ Tech Awards 2021.\n",
        "Frequently praised for excellent project management and leadership skills.\n",
        "Speaker at the ABC Developer's conference on 'Innovations in Machine Learning'.\n",
        "Promoted to Senior Developer within two years of tenure at CompanyX.\n",
        "\"\"\"\n",
        "resume_content = result\n",
        "\n",
        "\n",
        "keywords = [\"achievement\", \"award\", \"recognition\", \"honor\", \"prize\", \"accolade\", \"commendation\", \"distinction\", \"kudos\", \"laurel\"]\n",
        "\n",
        "# Load the sentence transformer model\n",
        "model = SentenceTransformer('all-roberta-large-v1')  # or use 'all-MiniLM-L6-v2' for faster performance\n",
        "\n",
        "# Encode the keywords and the resume content into vectors\n",
        "keyword_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
        "resume_sentences = [sentence.strip() for sentence in resume_content.split('\\n') if sentence.strip()]  # Remove empty lines and strip spaces\n",
        "resume_embeddings = model.encode(resume_sentences, convert_to_tensor=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "l9RNmQn58jkY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the sentences with the highest semantic similarity to the keywords\n",
        "threshold = 0.2  # Set a threshold for semantic similarity\n",
        "achievements = []\n",
        "\n",
        "# Compute similarity scores\n",
        "similarity_scores = util.pytorch_cos_sim(resume_embeddings, keyword_embeddings)\n",
        "\n",
        "# Extract top similarity scores per sentence\n",
        "top_similarity_scores, top_idxs = torch.max(similarity_scores, dim=1)\n",
        "\n",
        "# Select sentences with similarity above the threshold\n",
        "for sentence, score in zip(resume_sentences, top_similarity_scores):\n",
        "    if score > threshold:\n",
        "        achievements.append((sentence, score.item()))\n",
        "\n",
        "# Sort achievements by their score for better relevance\n",
        "achievements.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print top 3 achievements/awards or most relevant sentences\n",
        "print(\"Achievements and Awards found in the resume:\")\n",
        "for idx, (achievement, score) in enumerate(achievements[:3], start=1):  # Limit to top 3 results\n",
        "    print(f\"{idx}. {achievement} (Score: {score:.4f})\")\n",
        "\n",
        "# If there are fewer than 3 achievements found, show the next most relevant sentences\n",
        "if len(achievements) < 3:\n",
        "    print(\"\\nNot enough achievements or awards found. Showing most relevant sentences:\")\n",
        "    sorted_sentences = sorted([(sentence, score.item()) for sentence, score in zip(resume_sentences, top_similarity_scores)], key=lambda pair: pair[1], reverse=True)\n",
        "    for idx, (sentence, score) in enumerate(sorted_sentences[:3], start=1):  # Take top 3 relevant sentences if achievements are less than 3\n",
        "        if sentence not in [achievement[0] for achievement in achievements]:  # Avoid repeating sentences already listed as achievements\n",
        "            print(f\"{idx}. {sentence} (Score: {score:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8rRXUrX9_CU",
        "outputId": "ceed39d3-c73e-47d0-eb51-03e50d3008cb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Achievements and Awards found in the resume:\n",
            "1. 2023 • Special Achievement Award by TCS – Employee of week during award Sprint. (Score: 0.5106)\n",
            "2. ACTIVITIES & ACHIEVEMENTS • Mentor at Grace Hopper Conference 2023. (Score: 0.3861)\n",
            "3. 2019 • 3rd Prize Winner at App Challenge – tech fest of VIT, Launched an ecommerce app in less than 12 hrs. (Score: 0.3173)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0TldrmC_S_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inserting into DB**"
      ],
      "metadata": {
        "id": "3FWwYWdk_UcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "# Your MongoDB URI\n",
        "uri = \"mongodb+srv://kashyapsoni3010:kashyapsoni3010@cluster0.ba1iivd.mongodb.net/?retryWrites=true&w=majority\"\n",
        "\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri)\n",
        "\n",
        "# Select the \"admitwise\" database\n",
        "db = client.admitwise\n",
        "\n",
        "# Select the \"ms_cse\" collection\n",
        "collection = db.data\n",
        "\n",
        "# Fetch all rows/documents from the collection\n",
        "cursor = collection.find()\n",
        "\n",
        "# Convert the cursor to a list of documents\n",
        "documents = list(cursor)\n",
        "\n",
        "# Now, 'documents' contains all the rows from the \"ms_cse\" collection\n",
        "for document in documents:\n",
        "    print(document)\n",
        "\n",
        "# Close the MongoDB connection when done\n",
        "client.close()\n"
      ],
      "metadata": {
        "id": "jJ1PMTMV_SyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_insert_naman = {\n",
        "  \"id\":30124521,\n",
        "  \"name\":\"Naman Khurpia\",\n",
        "  \"email\":\"namankhurpia2@gmail.com\",\n",
        "  \"date\":\"12/03/2023\",\n",
        "  \"notes\":\"Good candidate, can be considered for Acceptance, Has good work experience and graduated from reputed university\",\n",
        "  \"uniValues\":{\n",
        "    \"val1\":4,\n",
        "    \"val2\":3,\n",
        "    \"val3\":5,\n",
        "    \"val4\":4\n",
        "  },\n",
        "  \"departmentValues\":{\n",
        "    \"val1\":5,\n",
        "    \"val2\":4,\n",
        "    \"val3\":3,\n",
        "    \"val4\":4\n",
        "  },\n",
        "  \"keyStats\":{\n",
        "    \"workexp\":\" 2 years at Tata Consultancy Services\",\n",
        "    \"university\" : \"Vellore Institute of Technology\",\n",
        "    \"GPA\":\"8.5 / 10.0\",\n",
        "    \"GRE\":\"318 / 340\",\n",
        "    \"IELTS\":\"7.5 / 9.0\",\n",
        "    \"achievement1\": \"Special Achievement Award by TCS – Employee of week during award Sprint. (Score: 0.5106)\",\n",
        "    \"achievement2\":\"Mentor at Grace Hopper Conference 2023\",\n",
        "    \"achievement3\":\"3rd Prize Winner at App Challenge – tech fest of VIT, Launched an ecommerce app in less than 12 hrs\"\n",
        "  },\n",
        "  \"undergradscores\":{\n",
        "    \"Algorithms\":\"[98,87,56,98]\",\n",
        "    \"Computer Science\":\"[48,97,76,88]\",\n",
        "    \"Operating System\":\"[32,55,88,56]\",\n",
        "    \"GPA Overall\": \"[89,58,78,93]\"\n",
        "  },\n",
        "  \"files\":{\n",
        "    \"resume\":\"link\",\n",
        "    \"sop\":\"<p><span style=\\\"background-color: yellow;\\\">Adam Koehler Statement of Purpose After taking a multitude of computer science courses over my academic career I have perceived that most of the courses tend to build a student’s knowledge of computer science outward, broadening familiarity over several areas of computer science rather than creating a concentrated depth of knowledge in any one topic.</span> <span style=\\\"background-color: yellow;\\\">Pursuing a master’s degree in computer science will allow me to more fully develop my knowledge in two areas of particular interest to me: artificial intelligence and computer graphics.</span> <span style=\\\"background-color: yellow;\\\">By studying these areas in depth, I hope to resolve the disparity between having a wide scope of computer science knowledge and having a more concentrated understanding of a single computer science field.</span> Artificial intelligence is the foremost area of interest that I wish study further. <span style=\\\"background-color: yellow;\\\">The attraction of artificial intelligence for me lies in its breadth of applicability, both as a method of problem solving in itself and in a symbiotic integration with other areas of computer science.</span> A broad spectrum of applications exist within the artificial intelligence field, ranging from intelligent non-player controlled characters in computer game software to a ubiquitous computing solution that intelligently reacts to a variety of users. This diversity is one of the main reasons that I feel compelled to pursue artificial intelligence further. While I have striven to develop my understanding of artificial intelligence during my undergraduate education, the choreographed requirements of a bachelor's degree have restricted my research to only a minute sample of artificial intelligence’s applications. During my exposure to the field, I have often been unsatisfied with the level of interaction artificial intelligence displays in response to prompts of varying complexity. <span style=\\\"background-color: yellow;\\\">I do not believe the field has been developed to its potential in any regard, and feel that considerable progress can be made to improve the interactive experience that users have with an artificial intelligence application.</span> <span style=\\\"background-color: yellow;\\\">This genuine intrigue combined with my curiosity for the subject matter and the limitless potential of the field are the reason why I wish to pursue a greater depth of knowledge in artificial intelligence.</span> Through the education gained in pursuit of a master’s degree, I hope to be able to enrich the authenticity of many artificial intelligence experiences, from computer games, to interactive toys and beyond. While artificial intelligence holds the most intrigue for me, a secondary area of interest is computer graphics. Akin to artificial intelligence in that I have only touched on the subject during my undergraduate career, I hope to explore the areas of two and three dimensional rendering more acutely. <span style=\\\"background-color: yellow;\\\">While I have always enjoyed the freedom of creative expression, and embrace its value in many aspects of problem solving in computer science, the rigidity of programming has precluded my pursuit of many artistic interests.</span> I feel that a more in-depth review of the field of computer graphics would be very fulfilling for me, both for its creative liberties and for its application to other areas of computer science. Specifically with regard to my interest in artificial intelligence, I envision graphics applications such as an artificially intelligent avatar, with body and facial expressions that can create more engaging interaction between two users. It is also important to understand just how quickly the field of computer graphics is changing within computer science. I believe that a deeper appreciation for the state-of-the-art in computer graphics will only help me in all future computer science pursuits, and I look forward to the new challenges while pursuing my master’s degree in computer science. My undergraduate education has prepared me for the depth and commitment required of graduate research in the field of computer science. Two projects I have participated in would be research with the Embedded XINU team, and a senior design project also involving the XINU operating system. While participating on the Embedded XINU team I have chosen to aid in the research and development of an external analysis tool for embedded systems development. <span style=\\\"background-color: yellow;\\\">Personally, I have researched integrating the front end system with the debugging system, developed by another team member, via an open source debugging software.</span> <span style=\\\"background-color: yellow;\\\">Once complete, it will allow users to access many debugging features, such as single stepping through code, breaking at a certain point in the code, and continuing the execution of the code from the breakpoint.</span> <span style=\\\"background-color: yellow;\\\">The senior design project involves the creation of a software program to allow users building a XINU operating system to directly use computers with a Windows operating system installed.</span> Currently, a direct or remote connection to a UNIX variant needs to be used to aid development. Both projects have allowed me to see the considerable amount of work and research that goes into software development projects. Each of the projects is team oriented; with tasks split down so that one or more researchers can contribute. The groups function with relative autonomy, without daily guidance of a professor, although guidance is provided on a semi-periodic basis. Overall, these projects have provided useful experiences allowing me to understand both graduate and undergraduate level research and implementation. <span style=\\\"background-color: yellow;\\\">In addition to these two XINU based projects, I have also completed several course-based projects that assimilate the entire semester's work into a cohesive project.</span> One such project was a language interpreter written in Scheme that interprets primitive operations including add, subtract, Boolean not, and, or, and xor; as well as complex statements such as variable assignment, print statements, procedure declarations and calls and simple type checking. This and several other projects have helped me develop a wealth of knowledge in the computer science field, and have certainly broadened my interests. <span style=\\\"background-color: yellow;\\\">However, they have lacked depth in certain areas that I feel truly drawn to exploring.</span> Through graduate studies I hope to explore the fields of artificial intelligence and computer graphics more fully, and negotiate the requirements of intertwining the two more seamlessly than today's efforts produce. My ultimate goal is to produce meaningful work that combines the acquired in-depth knowledge of both the artificial intelligence field and the computer graphics field, while building on the solid foundation I have created in my undergraduate pursuits. </p>\",\n",
        "    \"transcript\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/consolidated.pdf\",\n",
        "    \"lor1\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/Letter_of_Recommendation_-_koushik_dutta.pdf\",\n",
        "    \"lor2\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/Letter_of_Recommendation_-_koushik_dutta.pdf\",\n",
        "    \"lor3\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/Letter_of_Recommendation_-_koushik_dutta.pdf\",\n",
        "  \"gre\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/cao_b2.pdf\",\n",
        "    \"ielts\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/cao_b2.pdf\"\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "data_to_insert_dhaval = {\n",
        "  \"id\":30124521,\n",
        "  \"name\":\"Dhaval Patel\",\n",
        "  \"email\":\"dhavaljpatel@gmail.com\",\n",
        "  \"date\":\"14/02/2023\",\n",
        "  \"notes\":\"Good candidate, can be considered for Acceptance, Has good work experience and graduated from reputed university\",\n",
        "  \"uniValues\":{\n",
        "    \"val1\":5,\n",
        "    \"val2\":3,\n",
        "    \"val3\":3,\n",
        "    \"val4\":2\n",
        "  },\n",
        "  \"departmentValues\":{\n",
        "    \"val1\":2,\n",
        "    \"val2\":5,\n",
        "    \"val3\":4,\n",
        "    \"val4\":3\n",
        "  },\n",
        "  \"keyStats\":{\n",
        "    \"workexp\":\" 5 years at Colgate\",\n",
        "    \"university\" : \"University of Mumbai\",\n",
        "    \"GPA\":\"9.0 / 10.0\",\n",
        "    \"GRE\":\"323 / 340\",\n",
        "    \"IELTS\":\"8.0 / 9.0\",\n",
        "    \"achievement1\": \"Best Employee Award by Colgate for 6 months\",\n",
        "    \"achievement2\":\"3rd prize winner at MLH Hack held at NIT Roorkee\",\n",
        "    \"achievement3\":\"3rd Prize Winner at Web Challenge – tech fest of NIT\"\n",
        "  },\n",
        "  \"undergradscores\":{\n",
        "    \"Algorithms\":\"[48,57,96,88]\",\n",
        "    \"Computer Science\":\"[48,67,96,88]\",\n",
        "    \"Operating System\":\"[32,35,28,56]\",\n",
        "    \"GPA Overall\": \"[99,28,38,93]\"\n",
        "  },\n",
        "  \"files\":{\n",
        "    \"resume\":\"link\",\n",
        "    \"sop\":\"<p><span style=\\\"background-color: yellow;\\\">Statement of Purpose\\n\\nSiddharth Sharma\\n\\nOctober 26, 2018\\n\\nWhen I think about the different realms of science and technology, it is hard to think of a domain that is going to change the world as much as advanced software will, in the decades ahead.</span> <span style=\\\"background-color: yellow;\\\">If Ray Kurzweil is right in his book “Singularity is Near”, we are near the end of ‘Epoch 4’ (Technology) and entering into ‘Epoch 5’ (The Merger of Human Technology with Human Intelligence).</span> <span style=\\\"background-color: yellow;\\\">In the distant future, as Ray points out in his book - advanced technology will enable us to have an artificial heart, knees, and almost any other part of the human body.</span> Paralysis due to spinal injuries could be overcome by using specialized electronics that would allow paraplegics to regain control over their muscles. While Ray’s ideas are interesting and exciting to think about, in the near future I foresee computer science being used as a tool, as mathematics is used, for all the fields of science. It has always been important for physicists, chemists and biologists to have some understanding of specific areas of mathematics to express their ideas, to write down formulas, and to make predictions. <span style=\\\"background-color: yellow;\\\">Similarly, in the near future, computer software will become an integral part of the research to create new breakthroughs.</span> For instance, all fields of science collect and analyze a large amount of data. <span style=\\\"background-color: yellow;\\\">Finding patterns in large amounts of data has always been a stumbling block.</span> However, by using state of the art software for data mining and machine learning, hidden trends and patterns may emerge that can help increase understanding and solve problems. Similarly, with the vast amounts of data in genomics, we can use various data mining techniques to find patterns that advance our understanding of how the human body functions, including understanding some of the brain’s intricacies. These patterns could also help with disease diagnosis and possible cures. I feel excited about what Ray describes as ‘Epoch 5’ where the focus shifts toward merging technology with human intelligence to create an age of Artificial Intelligence. I want to contribute to that age of evolution. Thus, a M.S. <span style=\\\"background-color: yellow;\\\">degree in Computer Science (with emphasis on Artificial Intelligence) is the logical culmination of my passion for Artificial Intelligence and Computer Science in general.</span> I earned my undergraduate degree in Computer Science [Elective - Artificial Intelligence] at the University of Pune with the grade ‘First Class with Distinction’. Also, I was the topper of the undergraduate program at my school. Prior to my undergraduate degree, I have been the topper in Mathematics and Computer Science at my high school. In 2007, I received the ‘Best Student’ award for the highest achievement in academics and for my contributions to the school. <span style=\\\"background-color: yellow;\\\">During my first year in the undergraduate program, I co-founded the Association of Computer Engineering Students (ACE) at my college, and I headed the organization during my junior and senior years in the positions of The General Secretary.</span> <span style=\\\"background-color: yellow;\\\">Also in my senior year, I was the President of ‘Digital Renaissance’, the technical festival organized by the CS department.</span> It holds the record for the highest number of participants and sponsors in the history of the college. Over the course of my undergraduate program, I participated in various programming contests and won many of them. I always enjoyed the practical aspects of my studies, and that is one of the reasons why my marks in lab exercises, term work, oral examinations, practical examinations, and projects averaged above 85%, the highest in my school. I also developed various small software tools for my school, such as a web-based system to conduct online examinations, an online course repository, and a small lightweight console for remote manageability of desktops. 1 During the final year of my undergraduate degree, I received an internship opportunity at BMC Software, to work on their Marimba products, the product written by the creators of the Java language (Arthur van Hoff, Jonathan Payne, and Sami Shaio). <span style=\\\"background-color: yellow;\\\">At the end of my internship, I was offered to join the Marimba core R&D team.</span> <span style=\\\"background-color: yellow;\\\">While working on Marimba, I learned various aspects of building large and scalable systems, protocols and algorithms for efficient and secure content distribution over network, and managing a heterogeneous infrastructure.</span> <span style=\\\"background-color: yellow;\\\">I was also actively involved in the research and design of a self-enabling infrastructure for Intel AMT Agent Presence, an efficient data structure for sending large statistics reports over WAN, and better management of OS runtime processes.</span> I have written white-papers and technical documents which have been incorporated into the products. <span style=\\\"background-color: yellow;\\\">At BMC, I developed many of the key Marimba features such as ‘Marimba’s support to Windows Server 2008 and Windows 7’, ‘Marimba’s package deployment support to x64 architecture (native x64 packaging support)’, ‘CRS(Common Reboot Service) framework’, ‘Policy-based power management’, ‘Intel AMT support’; ‘Marimba AMT Java SDK for Channel developers’, ‘Session migration of marimba client to better manage installation of user application when migrating from session 0’, ‘Certificate management’ and ‘Code signing module of Marimba (rewrote in order to move from using phaos library to RSA library)’.</span> I also played a major role in porting Marimba Client from Java Runtime 1.4 to Java Runtime 1.6 on Windows, Linux, Solaris, HP-UX and AIX. Throughout my stay at BMC, I had the opportunity to mentor many undergraduate and graduate interns. <span style=\\\"background-color: yellow;\\\">I mentored a few academic projects, mostly in collaboration with the CS department at my undergraduate school.</span> I also delivered many guest lectures. <span style=\\\"background-color: yellow;\\\">A few months ago, I founded a company, Cathysoft, with the idea of writing a clinically useful program to overcome the limitations of conventional computer-aided diagnosis.</span> <span style=\\\"background-color: yellow;\\\">However, after doing some initial research on simulation of human reasoning based on patient data for diagnosing diseases – I realized that I need to gain more experience and exposure to an excellent research environment.</span> Thus I decided to pursue a masters degree. <span style=\\\"background-color: yellow;\\\">I want to make sure that the company I am building is around for a long term and with this goal in mind I decided to study further in an institute that help me further my goals.</span> I am looking for a school that has the world-class study and research atmosphere, the best faculty, exceptional peers, and excellent infrastructure, an institute that can facilitate resources to help attain my ambitions. Stanford University has the unique distinction of being at the forefront of innovation and producing entrepreneurs. <span style=\\\"background-color: yellow;\\\">The university is the birthplace for the companies like Google, Yahoo, Sun Microsystems, HP, Cisco, Intel, VMWare, Electronic Arts, and many more..., and is known in the world as ‘The Place for Startups’.</span> Thus, Stanford University is my choice for the graduate studies. I am very optimistic about the future of my start-up, by being there. <span style=\\\"background-color: yellow;\\\">At Stanford, I hope to get an opportunity to meet the top management of some international high-tech start-ups and gain hands-on experience in starting and running a new enterprise.</span> It would help me understand the issues and policies that affect the climate for innovation and start-up success around the world. 2 </p>\",\n",
        "    \"transcript\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/consolidated.pdf\",\n",
        "    \"lor1\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/cao_b2.pdf\",\n",
        "    \"lor2\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/IWP_CAT-1_E1_Slot_Question__Key.pdf\",\n",
        "    \"lor3\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/Letter_of_Recommendation_-_koushik_dutta.pdf\",\n",
        "  \"gre\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/web_mining_cat_1.pdf\",\n",
        "    \"ielts\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/cao_b2.pdf\"\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "data_to_insert_kashyap = {\n",
        "  \"id\":30124521,\n",
        "  \"name\":\"Kashyap Soni\",\n",
        "  \"email\":\"kashyapsoni@gmail.com\",\n",
        "  \"date\":\"18/03/2023\",\n",
        "  \"notes\":\"Good candidate, can be considered for Acceptance, graduated from reputed university\",\n",
        "  \"uniValues\":{\n",
        "    \"val1\":5,\n",
        "    \"val2\":4,\n",
        "    \"val3\":1,\n",
        "    \"val4\":2\n",
        "  },\n",
        "  \"departmentValues\":{\n",
        "    \"val1\":4,\n",
        "    \"val2\":5,\n",
        "    \"val3\":1,\n",
        "    \"val4\":3\n",
        "  },\n",
        "  \"keyStats\":{\n",
        "    \"workexp\":\"None\",\n",
        "    \"university\" : \"National Institute of Technology\",\n",
        "    \"GPA\":\"9.5 / 10.0\",\n",
        "    \"GRE\":\"329 / 340\",\n",
        "    \"IELTS\":\"7.5 / 9.0\",\n",
        "    \"achievement1\": \"1st Prize winner at NIT HACK, created UPI money tranfer app\",\n",
        "    \"achievement2\":\"3rd prize winner at MLH Hack held at NIT Roorkee\",\n",
        "    \"achievement3\":\"3rd Prize Winner at Web Challenge – tech fest of NIT\"\n",
        "  },\n",
        "  \"undergradscores\":{\n",
        "    \"Algorithms\":\"[98,87,36,88]\",\n",
        "    \"Computer Science\":\"[48,37,96,88]\",\n",
        "    \"Operating System\":\"[32,95,28,96]\",\n",
        "    \"GPA Overall\": \"[11,28,68,93]\"\n",
        "  },\n",
        "  \"files\":{\n",
        "    \"resume\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/NamanKhurpiaResume.pdf\",\n",
        "    \"sop\":\"<p><span style=\\\"background-color: yellow;\\\">STATEMENT OF PURPOSE NAME : jinson.j.erinjeri Applying for M.S.</span> <span style=\\\"background-color: yellow;\\\">IN INDUSTRIAL ENGINEERING Firstly I would like to introduce myself as an under graduate student of the bachelor of engineering program From the M.S.Ramaiah institute of technology, one of the most prestigious institutions of the Bangalore U university.</span> I have completed my course of engineering in the field of 'Industrial Engineering’, which was for a period of four years. I graduated in the first class with distinction. I was ranked among the top five in the class. During these four years of my under graduate course, I gained in-depth understanding of the various techniques involved in problem solving, mainly to cater to the services of the industries. Manufacturing Processes, Operations Research, Industrial Management, Quantitative techniques were the other subjects which enabled me to blend the required action whenever an problem was posed to me. Behavioral Science was the most interesting subject which I mastered during this four year period. This subject provided me with valuable information which helped me to develop leadership skills. The factor of empathy really coerced me to instigate leadership skills.. <span style=\\\"background-color: yellow;\\\">Since computers have become inseparable part of INDUSTRIAL ENGINEERING I deemed it fit to learn more about computers and I am presently doing my course in C,C++.</span> I have worked on software packages like MS Project, LINDO, LOTUS 123. To meet my B.E. Degree requirement, I executed a project titled 'REDUCTION OF LEAD CYCLE FOR MANUFACTURE OF STEAM TURBINES' under the expert guidance of N.V.R.Naidu and this project was presented in the O.R. <span style=\\\"background-color: yellow;\\\">Society of India.</span> The main aim of the project was to reduce the lead cycle of manufacture of a particular class of turbine so as to reduce the inventory costs and enable the industry to forecast accurately for the period ahead. <span style=\\\"background-color: yellow;\\\">The techniques involved LINE OF BALANCE METHOD for scheduling and Controlling and Post college, I AM presently working in KIRLOSKAR ELECTRIC COMPANY WHERE IN I am going through a rigorous training program which will be completed very shortly.</span> The under graduate course as well as my training at KIRLOSKAR ELECTRIC COMPANY has provided me with a strong base for further growth in any of my desired fields. I would like to delve deeper into the fields of my choice and their technical aspects completely. I hope to acquire the requisite professional skills and develop a thorough understanding in these following areas. I wish to contribute towards these areas and indulge in research which ultimately should have a meaningful contribution to science and technology. <span style=\\\"background-color: yellow;\\\">I AM confident that my academic capability and analytical skills coupled with my perseverance and single-minded devotion will see me through to this goal.</span> <span style=\\\"background-color: yellow;\\\">To this end, the first step is a sound graduate study.</span> I have chosen the graduate program to further my interests. <span style=\\\"background-color: yellow;\\\">It is my belief that knowledge gained has to be shared.</span> <span style=\\\"background-color: yellow;\\\">I believe that imparting knowledge is an enjoyable and satisfying experience.</span> I have enjoyed giving ideas, lectures and making presentations on technical/non-technical topics at college and at the workplace. <span style=\\\"background-color: yellow;\\\">Hence, I am eager to obtain An assistantship. Consequently, I understand that the choice of the University is of paramount importance.</span> <span style=\\\"background-color: yellow;\\\">After perusing your brochures and consulting my professors, I reached the conclusion that the with its reputed faculty, excellent facilities, and tradition of academic excellence will be the ideal place to work towards the fulfillment of my goal.</span> <span style=\\\"background-color: yellow;\\\">Moreover, I am confident that the wholesome education that I will receive at the will stand me in good stead throughout my career.</span> <span style=\\\"background-color: yellow;\\\">Thanking you,</span> </p>\"\n",
        ",\n",
        "    \"transcript\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/consolidated.pdf\",\n",
        "    \"lor1\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/cao_b2.pdf\",\n",
        "    \"lor2\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/IWP_CAT-1_E1_Slot_Question__Key.pdf\",\n",
        "    \"lor3\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/Letter_of_Recommendation_-_koushik_dutta.pdf\",\n",
        "  \"gre\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/web_mining_cat_1.pdf\",\n",
        "    \"ielts\":\"https://namankhurpia.pythonanywhere.com/media/catonefiles/cao_b2.pdf\"\n",
        "  }\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "iR5QDAjB_YCx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}